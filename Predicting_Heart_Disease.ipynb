{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":125192,"databundleVersionId":15408205,"sourceType":"competition"}],"dockerImageVersionId":31259,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-13T06:45:38.257098Z","iopub.execute_input":"2026-02-13T06:45:38.257659Z","iopub.status.idle":"2026-02-13T06:45:39.903289Z","shell.execute_reply.started":"2026-02-13T06:45:38.257590Z","shell.execute_reply":"2026-02-13T06:45:39.901395Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/playground-series-s6e2/sample_submission.csv\n/kaggle/input/playground-series-s6e2/train.csv\n/kaggle/input/playground-series-s6e2/test.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport warnings\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Sklearn for preprocessing and evaluation\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.metrics import log_loss\nfrom sklearn.impute import SimpleImputer\n\n# Models\nimport xgboost as xgb\nimport lightgbm as lgb\nimport catboost as cb\n\n# Configuration\nwarnings.filterwarnings('ignore')\npd.set_option('display.max_columns', None)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T06:46:08.745804Z","iopub.execute_input":"2026-02-13T06:46:08.746129Z","iopub.status.idle":"2026-02-13T06:46:19.232684Z","shell.execute_reply.started":"2026-02-13T06:46:08.746104Z","shell.execute_reply":"2026-02-13T06:46:19.231744Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Load the dataset\ndf = pd.read_csv('/kaggle/input/playground-series-s6e2/train.csv')\n\n# 1. Cleaning: Drop unnecessary ID column\nif 'id' in df.columns:\n    df = df.drop('id', axis=1)\n\n# 2. Cleaning: Remove Duplicates\ninitial_len = len(df)\ndf = df.drop_duplicates()\nprint(f\"Removed {initial_len - len(df)} duplicate rows.\")\n\n# 3. Cleaning: Handle Null Values\n# Although your dataset seems clean, this is a robust step for production code.\n# We separate features and target first to avoid imputing the target.\ntarget_col = 'Heart Disease'\nfeatures = [c for c in df.columns if c != target_col]\n\n# Impute missing values (using mean for simplicity, or median/mode)\nimputer = SimpleImputer(strategy='mean')\ndf[features] = imputer.fit_transform(df[features])\n\nprint(\"Data Shape after cleaning:\", df.shape)\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T06:46:24.338874Z","iopub.execute_input":"2026-02-13T06:46:24.340060Z","iopub.status.idle":"2026-02-13T06:46:25.755252Z","shell.execute_reply.started":"2026-02-13T06:46:24.340010Z","shell.execute_reply":"2026-02-13T06:46:25.754058Z"}},"outputs":[{"name":"stdout","text":"Removed 0 duplicate rows.\nData Shape after cleaning: (630000, 14)\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"    Age  Sex  Chest pain type     BP  Cholesterol  FBS over 120  EKG results  \\\n0  58.0  1.0              4.0  152.0        239.0           0.0          0.0   \n1  52.0  1.0              1.0  125.0        325.0           0.0          2.0   \n2  56.0  0.0              2.0  160.0        188.0           0.0          2.0   \n3  44.0  0.0              3.0  134.0        229.0           0.0          2.0   \n4  58.0  1.0              4.0  140.0        234.0           0.0          2.0   \n\n   Max HR  Exercise angina  ST depression  Slope of ST  \\\n0   158.0              1.0            3.6          2.0   \n1   171.0              0.0            0.0          1.0   \n2   151.0              0.0            0.0          1.0   \n3   150.0              0.0            1.0          2.0   \n4   125.0              1.0            3.8          2.0   \n\n   Number of vessels fluro  Thallium Heart Disease  \n0                      2.0       7.0      Presence  \n1                      0.0       3.0       Absence  \n2                      0.0       3.0       Absence  \n3                      0.0       3.0       Absence  \n4                      3.0       3.0      Presence  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Age</th>\n      <th>Sex</th>\n      <th>Chest pain type</th>\n      <th>BP</th>\n      <th>Cholesterol</th>\n      <th>FBS over 120</th>\n      <th>EKG results</th>\n      <th>Max HR</th>\n      <th>Exercise angina</th>\n      <th>ST depression</th>\n      <th>Slope of ST</th>\n      <th>Number of vessels fluro</th>\n      <th>Thallium</th>\n      <th>Heart Disease</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>58.0</td>\n      <td>1.0</td>\n      <td>4.0</td>\n      <td>152.0</td>\n      <td>239.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>158.0</td>\n      <td>1.0</td>\n      <td>3.6</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>7.0</td>\n      <td>Presence</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>52.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>125.0</td>\n      <td>325.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>171.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>Absence</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>56.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>160.0</td>\n      <td>188.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>151.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>Absence</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>44.0</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>134.0</td>\n      <td>229.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>150.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>Absence</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>58.0</td>\n      <td>1.0</td>\n      <td>4.0</td>\n      <td>140.0</td>\n      <td>234.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>125.0</td>\n      <td>1.0</td>\n      <td>3.8</td>\n      <td>2.0</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>Presence</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# 1. Identify Feature Types\n# Based on the dataset: Age, BP, Cholesterol, Max HR, ST depression are continuous.\n# The rest are categorical/ordinal integers.\nnum_cols = ['Age', 'BP', 'Cholesterol', 'Max HR', 'ST depression']\ncat_cols = [c for c in features if c not in num_cols]\n\nprint(f\"Numerical Features: {num_cols}\")\nprint(f\"Categorical Features: {cat_cols}\")\n\n# 2. Feature Scaling\n# We scale numerical features so they have mean 0 and variance 1.\nscaler = StandardScaler()\ndf[num_cols] = scaler.fit_transform(df[num_cols])\n\n# 3. Target Encoding\n# Convert 'Presence'/'Absence' to 1/0\nle = LabelEncoder()\ndf[target_col] = le.fit_transform(df[target_col])\n\nprint(f\"Target Classes: {le.classes_}\")\n# X and y for training\nX = df[features]\ny = df[target_col]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T06:46:57.721253Z","iopub.execute_input":"2026-02-13T06:46:57.721599Z","iopub.status.idle":"2026-02-13T06:46:57.987002Z","shell.execute_reply.started":"2026-02-13T06:46:57.721574Z","shell.execute_reply":"2026-02-13T06:46:57.986176Z"}},"outputs":[{"name":"stdout","text":"Numerical Features: ['Age', 'BP', 'Cholesterol', 'Max HR', 'ST depression']\nCategorical Features: ['Sex', 'Chest pain type', 'FBS over 120', 'EKG results', 'Exercise angina', 'Slope of ST', 'Number of vessels fluro', 'Thallium']\nTarget Classes: ['Absence' 'Presence']\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Configuration for Cross-Validation\nN_SPLITS = 5\nskf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n\n# Arrays to store Out-of-Fold (OOF) predictions for the entire dataset\noof_preds_xgb = np.zeros(len(X))\noof_preds_lgb = np.zeros(len(X))\noof_preds_cb = np.zeros(len(X))\noof_preds_ensemble = np.zeros(len(X))\n\n# Lists to store log loss scores per fold\nscores_xgb = []\nscores_lgb = []\nscores_cb = []\nscores_ensemble = []\n\nprint(f\"Starting Training with {N_SPLITS}-Fold Cross Validation...\\n\")\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n    X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n    X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n    \n    # --- Model 1: XGBoost ---\n    model_xgb = xgb.XGBClassifier(\n        n_estimators=1000,\n        learning_rate=0.05,\n        max_depth=6,\n        eval_metric='logloss',\n        early_stopping_rounds=50,\n        random_state=42,\n        use_label_encoder=False\n    )\n    model_xgb.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n    val_pred_xgb = model_xgb.predict_proba(X_val)[:, 1]\n    oof_preds_xgb[val_idx] = val_pred_xgb\n    loss_xgb = log_loss(y_val, val_pred_xgb)\n    scores_xgb.append(loss_xgb)\n\n    # --- Model 2: LightGBM ---\n    model_lgb = lgb.LGBMClassifier(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=31,\n        random_state=42,\n        verbosity=-1\n    )\n    # LightGBM requires early stopping via callbacks in newer versions or explicit param\n    callbacks = [lgb.early_stopping(stopping_rounds=50, verbose=False)]\n    model_lgb.fit(X_train, y_train, eval_set=[(X_val, y_val)], callbacks=callbacks)\n    val_pred_lgb = model_lgb.predict_proba(X_val)[:, 1]\n    oof_preds_lgb[val_idx] = val_pred_lgb\n    loss_lgb = log_loss(y_val, val_pred_lgb)\n    scores_lgb.append(loss_lgb)\n\n    # --- Model 3: CatBoost ---\n    model_cb = cb.CatBoostClassifier(\n        iterations=1000,\n        learning_rate=0.05,\n        depth=6,\n        eval_metric='Logloss',\n        random_seed=42,\n        verbose=0,\n        allow_writing_files=False\n    )\n    model_cb.fit(X_train, y_train, eval_set=(X_val, y_val), early_stopping_rounds=50)\n    val_pred_cb = model_cb.predict_proba(X_val)[:, 1]\n    oof_preds_cb[val_idx] = val_pred_cb\n    loss_cb = log_loss(y_val, val_pred_cb)\n    scores_cb.append(loss_cb)\n\n    # --- Ensemble: Simple Probability Averaging ---\n    val_pred_ensemble = (val_pred_xgb + val_pred_lgb + val_pred_cb) / 3\n    oof_preds_ensemble[val_idx] = val_pred_ensemble\n    loss_ensemble = log_loss(y_val, val_pred_ensemble)\n    scores_ensemble.append(loss_ensemble)\n\n    print(f\"Fold {fold+1} | XGB: {loss_xgb:.5f} | LGB: {loss_lgb:.5f} | CB: {loss_cb:.5f} | Ensemble: {loss_ensemble:.5f}\")\n\nprint(\"\\n\" + \"=\"*50)\nprint(f\"Mean Log-Loss XGBoost:  {np.mean(scores_xgb):.5f}\")\nprint(f\"Mean Log-Loss LightGBM: {np.mean(scores_lgb):.5f}\")\nprint(f\"Mean Log-Loss CatBoost: {np.mean(scores_cb):.5f}\")\nprint(f\"Mean Log-Loss Ensemble: {np.mean(scores_ensemble):.5f}\")\nprint(\"=\"*50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T06:47:29.172513Z","iopub.execute_input":"2026-02-13T06:47:29.173061Z","iopub.status.idle":"2026-02-13T06:56:17.012019Z","shell.execute_reply.started":"2026-02-13T06:47:29.173030Z","shell.execute_reply":"2026-02-13T06:56:17.010787Z"}},"outputs":[{"name":"stdout","text":"Starting Training with 5-Fold Cross Validation...\n\nFold 1 | XGB: 0.26777 | LGB: 0.26766 | CB: 0.26665 | Ensemble: 0.26700\nFold 2 | XGB: 0.27073 | LGB: 0.27075 | CB: 0.26976 | Ensemble: 0.27007\nFold 3 | XGB: 0.26813 | LGB: 0.26827 | CB: 0.26726 | Ensemble: 0.26752\nFold 4 | XGB: 0.26974 | LGB: 0.26976 | CB: 0.26870 | Ensemble: 0.26900\nFold 5 | XGB: 0.26718 | LGB: 0.26709 | CB: 0.26607 | Ensemble: 0.26642\n\n==================================================\nMean Log-Loss XGBoost:  0.26871\nMean Log-Loss LightGBM: 0.26871\nMean Log-Loss CatBoost: 0.26769\nMean Log-Loss Ensemble: 0.26800\n==================================================\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# --- 1. Load Data ---\ntest_df = pd.read_csv('/kaggle/input/playground-series-s6e2/test.csv')\nsample_submission = pd.read_csv('/kaggle/input/playground-series-s6e2/sample_submission.csv')\n\n# --- 2. Preprocessing Test Data ---\n# We must apply the exact same steps as we did for the training data\n# Store IDs for submission\ntest_ids = test_df['id']\n\n# Drop ID column from features\nif 'id' in test_df.columns:\n    test_df = test_df.drop('id', axis=1)\n\n# Handle Missing Values (using the same imputer as train)\n# Note: Ideally, you should fit the imputer on train and transform test.\n# Here, for simplicity in this section, we assume the 'imputer' and 'scaler' \n# from the previous section are available. \n# If not, we re-initialize them for demonstration (in a real pipeline, reuse the fitted objects).\nimputer = SimpleImputer(strategy='mean')\n# Fit on the full training features (X) to ensure we learn from the whole dataset\nimputer.fit(X) \ntest_df[features] = imputer.transform(test_df[features])\n\n# Scale Numerical Features (using the scaler fitted on train)\nscaler = StandardScaler()\nscaler.fit(X[num_cols]) # Re-fitting on full X just to be safe and robust\ntest_df[num_cols] = scaler.transform(test_df[num_cols])\n\n# --- 3. K-Fold Prediction with CatBoost ---\n# We use the same K-Fold split to train 5 models and average their predictions\ntest_preds_cumulative = np.zeros(len(test_df))\n\nprint(f\"Starting Prediction with {N_SPLITS}-Fold CatBoost Ensemble...\")\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n    X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n    X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n    \n    # Initialize CatBoost\n    model_cb = cb.CatBoostClassifier(\n        iterations=1000,\n        learning_rate=0.05,\n        depth=6,\n        eval_metric='Logloss',\n        random_seed=42,\n        verbose=0,\n        allow_writing_files=False\n    )\n    \n    # Train on the fold\n    model_cb.fit(X_train, y_train, eval_set=(X_val, y_val), early_stopping_rounds=50)\n    \n    # Predict on the TEST set (probabilities for class 1)\n    fold_preds = model_cb.predict_proba(test_df)[:, 1]\n    \n    # Add to cumulative predictions\n    test_preds_cumulative += fold_preds\n    \n    print(f\"Fold {fold+1} prediction completed.\")\n\n# Average the predictions across all folds\ntest_preds_avg = test_preds_cumulative / N_SPLITS\n\n# --- 4. Create Submission File ---\nsubmission = pd.DataFrame({\n    'id': test_ids,\n    'Heart Disease': test_preds_avg\n})\n\n# Save to CSV\nsubmission_filename = 'submission.csv'\nsubmission.to_csv(submission_filename, index=False)\n\nprint(f\"\\nSubmission file '{submission_filename}' created successfully!\")\nprint(submission.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T07:06:20.190032Z","iopub.execute_input":"2026-02-13T07:06:20.190598Z","iopub.status.idle":"2026-02-13T07:11:32.727444Z","shell.execute_reply.started":"2026-02-13T07:06:20.190561Z","shell.execute_reply":"2026-02-13T07:11:32.725995Z"}},"outputs":[{"name":"stdout","text":"Starting Prediction with 5-Fold CatBoost Ensemble...\nFold 1 prediction completed.\nFold 2 prediction completed.\nFold 3 prediction completed.\nFold 4 prediction completed.\nFold 5 prediction completed.\n\nSubmission file 'submission.csv' created successfully!\n       id  Heart Disease\n0  630000       0.535369\n1  630001       0.003979\n2  630002       0.789427\n3  630003       0.006078\n4  630004       0.304001\n","output_type":"stream"}],"execution_count":7}]}